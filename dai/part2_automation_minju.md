# Part 2 | AI 기반 운영 자동화 및 시스템 연동기

## 1. LLM이란?

LLM(Large Language Model)은 대규모 텍스트 데이터로 학습된 언어 모델임.
- 자연어를 이해하고 생성하는 AI
- GPT, Claude, Gemini 등이 대표적
- 문맥을 파악해 인간처럼 대화 가능

### LLM의 특징

```plaintext
1. 자연어 이해 (NLU)
- 사람의 의도 파악
- 문맥 기반 해석
- 다양한 표현 인식
```
```plaintext
2. 자연어 생성 (NLG)
- 문법적으로 올바른 문장 생성
- 스타일/톤 조절 가능
- 창의적 콘텐츠 작성
```
```plaintext
3. 범용성
- 번역, 요약, 분류, 코딩 등
- 별도 학습 없이 다양한 작업 수행
- 프롬프트만으로 기능 변경
```

### LLM vs 기존 AI

| LLM | 기존AI |
|---|---|
| "환불해주세요", "돈 돌려받고 싶어요", "취소하고 싶은데요"<br>→ 다양한 표현을 모두 "환불 의도"로 이해 | if "환불" in message:<br>&nbspreturn "환불 안내 페이지로 이동"<br>→ 미리 정의된 규칙만 처리 가능 |

## 2. 바이브 코딩과 LLM의 관계

바이브 코딩의 핵심은 LLM의 자연어 이해 능력임.
- 자연어 → 코드 변환이 LLM의 역할
- 개발자의 의도를 파악해 구현
- Cursor, Copilot 등이 내부적으로 LLM 활용

### 바이브 코딩의 작동 원리

<img width="722" height="352" alt="image" src="https://github.com/user-attachments/assets/beef34d2-dc0d-48d5-bd03-8cfed3f9844b" />

### LLM이 바이브 코딩을 가능하게 하는 이유

```plaintext
1. 코드 학습
- GitHub 등 대규모 코드 데이터로 학습
- 다양한 언어/프레임워크 이해
- 코딩 컨벤션과 패턴 습득
```
```plaintext
2. 문맥 이해
- 프로젝트 구조 파악
- 기존 코드 스타일 반영
- 연관된 파일 고려
```
```plaintext
3. 추론 능력
- 불완전한 설명도 보완
- 모범 사례 적용
- 에러 원인 분석
```

## 3. LLM을 활용한 자동화 적용 패턴

```plaintext
[분류 자동화]
텍스트 입력 → LLM 분류 → 라벨/카테고리 출력

[요약 자동화]
긴 문서 입력 → LLM 요약 → 핵심 내용 추출

[변환 자동화]
자연어 입력 → LLM 변환 → 코드/SQL/정규식 출력

[추출 자동화]
비정형 텍스트 → LLM 추출 → 구조화된 JSON 출력
```

## 4. LLM의 구조

### 트랜스포머 아키텍처

LLM의 핵심은 2017년 발표된 트랜스포머(Transformer) 구조임.

<img width="534" height="742" alt="image" src="https://github.com/user-attachments/assets/bb80b6ec-e291-49f3-ac23-ced15d4194eb" />

*출처: [AWS - 인공 지능에서 트랜스포머란 무엇인가요?](https://aws.amazon.com/ko/what-is/transformers-in-artificial-intelligence)*

### 핵심 개념

#### 1) 토큰화 (Tokenization)

```plaintext
입력: "안녕하세요, 반갑습니다"
      ↓
토큰: ["안녕", "하세요", ",", "반갑", "습니다"]
      ↓
숫자: [1234, 5678, 99, 2345, 6789]

→ LLM은 텍스트를 숫자(토큰)로 변환해 처리
```

#### 2) 어텐션 (Attention)

```plaintext
"그 고양이가 쥐를 잡았다. 그것은 빨랐다."

질문: "그것"은 무엇을 가리키나?

어텐션 메커니즘:
- "그것" ↔ "고양이" 연관도: 0.8
- "그것" ↔ "쥐" 연관도: 0.2

→ 문맥 속 단어 간 관계를 파악
```

#### 3) 파라미터 (Parameters)

```plaintext
모델 크기 비교
- GPT-2: 15억 개
- GPT-3: 1,750억 개
- GPT-4: 추정 1조+ 개
```
```plaintext
파라미터가 많을수록
- 더 복잡한 패턴 학습
- 더 정교한 응답 생성
- 더 많은 컴퓨팅 자원 필요
- 더 높은 비용
```

### LLM 작동 흐름

```plaintext
1. 입력 처리
   "오늘 날씨 어때?" → 토큰화 → 임베딩 벡터

2. 인코딩
   어텐션으로 문맥 파악 → 의미 표현 생성

3. 디코딩
   다음 토큰 확률 계산 → 가장 적절한 단어 선택

4. 출력 생성
   토큰 → 텍스트 변환 → "오늘 서울 날씨는 맑습니다"
```

### 프롬프트 엔지니어링의 원리

LLM은 "다음에 올 가장 적절한 토큰"을 예측함.

**프롬프트가 중요한 이유**
- 좋은 프롬프트 = 좋은 맥락 제공
- 맥락이 명확할수록 정확한 예측
- 예시를 주면 패턴을 따라함 (Few-shot)

```plaintext
[Zero-shot]
"다음 문장의 감정을 분석해줘: 정말 최악이야"

[Few-shot]
"감정 분석 예시:
 '너무 좋아요' → 긍정
 '별로예요' → 부정
 
 분석할 문장: '정말 최악이야' →"

→ Few-shot이 더 정확한 결과
```

## 5. LLM 활용 시 주의사항

### 1) 할루시네이션 (Hallucination)

```plaintext
⚠️ 잘못된 정보를 자신있게 생성

예시:
Q: "당근마켓 창업자가 누구야?"
A: "김슬아입니다" (실제: 김용현, 김재현)

대응:
- 중요 정보는 반드시 검증
- 출처 요청하기
- 사실 확인 프로세스 구축
```

### 2) 컨텍스트 윈도우 제한

```plaintext
⚠️ 한 번에 처리할 수 있는 텍스트 길이 제한

모델별 제한:
- GPT-4: 약 128K 토큰
- Claude 3: 약 200K 토큰

대응:
- 긴 문서는 청크로 분할
- 중요 내용 우선 배치
- 요약 후 처리
```

### 3) 비용

```plaintext
⚠️ API 호출마다 비용 발생

비용 구조:
- 입력 토큰 × 단가
- 출력 토큰 × 단가

최적화:
- 프롬프트 간결하게
- 캐싱 활용
- 적절한 모델 선택 (작은 모델 우선)
```

### 프롬프트 작성 방법

```plaintext
1. 명확한 지시
   ❌ "이거 분석해줘"
   ⭕ "다음 텍스트를 감정/카테고리/요약 3가지로 분석해줘"

2. 출력 형식 지정
   ❌ 자유 형식
   ⭕ "JSON 형식으로 응답해줘: {category, sentiment, summary}"

3. 예시 제공
   ❌ 설명만
   ⭕ 입력-출력 예시 2-3개 포함

4. 역할 부여
   ❌ 일반 요청
   ⭕ "너는 고객 서비스 전문가야. 다음 문의를 분류해줘"
```

> LLM은 강력한 도구지만 만능이 아님. AI를 도구로 활용하되, 최종 판단은 사람이 해야 함.

## 참고 자료

1. [당근 테크 블로그, "비개발자 구성원들의 AI 도전기"](https://medium.com/daangn/ai-툴-개발은-처음이라-당근-비개발자-구성원들의-ai-도전기-fb62d2a6c2f3)
2. [당근 블로그, "AI가 만든 파도 위에서 당근이 서핑하는 법"](https://about.daangn.com/blog/archive/당근-ai-프로덕트-조직문화-사용자경험)